# Creating a data lake for Sparkify's user and song activity data 

## Overview
As Sparkify's user base grows so does the need for their analytics team to efficiently query user data to harveest insights on what songs users are listening to. At present, Sparkify's data is is stored in S3 buckets, in directories of JSON logs on user activity and metadata on the songs in their app. In order to facilitate easy querying the Sparkify analytics team needs access to normalized tables on user activitiy. This project addresses that issue by migrating the data from JSON files in S3 to a data lake consisting on normalized parquet files in S3. 

## Parquet File Schema
Sparkify's data lake contains one fact table and four dimension tables stored as parquet files in S3 as shown below. 
! [Sparkify fact and dimension schema](/Sparkify fact and dimension schema.png)


## ETL process
Log data from directories of JSON data generated by the Sparkify app on user and song data is stored as JSON files in two S3 buckets. The JSON data is processed via a Spark script run in an EMR cluster. The script extracts the data from s3, transforms it, and writes the normalized tables back to s3 as a series of partitioned parquet files.

## Technologies used
1. Python 3
2. Amazon Cloud Services (EMR, S3, and EC2)
3. Jupyter Notebooks

## Files 
- etl.py This file process the data and inserts the data into tables that are stored in s3
- sparkify-data-lake.ipynb This notebook can be imported into a AWS EMR notebook and run 

## Runnig the scripts
1. Run ``python3 etl.py`` from the command line
2. Import sparkify-data-lake.ipynb into a AWS EMR notebook and run 

### Credits
Code written by Scott Millslagle